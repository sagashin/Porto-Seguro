{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595200, 130) (892816, 130)\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 4\n",
      "Fit LGBMClassifier fold 5\n",
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "Fit XGBClassifier fold 4\n",
      "Fit XGBClassifier fold 5\n",
      "Fit GradientBoostingClassifier fold 1\n",
      "Fit GradientBoostingClassifier fold 2\n",
      "Fit GradientBoostingClassifier fold 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "# Regularized Greedy Forest\n",
    "from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing (Forza Baseline)\n",
    "id_test = test['id'].values\n",
    "\n",
    "col = [c for c in train.columns if c not in ['id','target']]\n",
    "col = [c for c in col if not c.startswith('ps_calc_')]\n",
    "\n",
    "train = train.replace(-1, np.NaN)\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "train = train.fillna(-1)\n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "\n",
    "def transform(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c:\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "col = [c for c in train.columns if c not in ['id','target']]\n",
    "col = [c for c in col if not c.startswith('ps_calc_')]\n",
    "\n",
    "dups = train[train.duplicated(subset=col, keep=False)]\n",
    "\n",
    "train = train[~(train['id'].isin(dups['id'].values))]\n",
    "\n",
    "target_train = train['target']\n",
    "train = train[col]\n",
    "test = test[col]\n",
    "print(train.values.shape, test.values.shape)\n",
    "\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "#                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        results = cross_val_score(self.stacker, S_train, y, cv=10, scoring='roc_auc')\n",
    "        print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict_proba(S_test)[:,1]\n",
    "        return res\n",
    "\n",
    "\n",
    "        \n",
    "# LightGBM params\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1250\n",
    "lgb_params['max_depth'] = 10\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "\n",
    "\n",
    "# RandomForest params\n",
    "rf_params = {}\n",
    "rf_params['n_estimators'] = 200\n",
    "rf_params['max_depth'] = 6\n",
    "rf_params['min_samples_split'] = 70\n",
    "rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "\n",
    "# ExtraTrees params\n",
    "#et_params = {}\n",
    "#et_params['n_estimators'] = 155\n",
    "#et_params['max_features'] = 0.3\n",
    "#et_params['max_depth'] = 6\n",
    "#et_params['min_samples_split'] = 40\n",
    "#et_params['min_samples_leaf'] = 18\n",
    "\n",
    "\n",
    "# XGBoost params\n",
    "xgb_params = {}\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['learning_rate'] = 0.02\n",
    "xgb_params['n_estimators'] = 1000\n",
    "xgb_params['max_depth'] = 4\n",
    "xgb_params['subsample'] = 0.9\n",
    "xgb_params['colsample_bytree'] = 0.9  \n",
    "xgb_params['min_child_weight'] = 10\n",
    "\n",
    "\n",
    "# CatBoost params\n",
    "cat_params = {}\n",
    "cat_params['iterations'] = 900\n",
    "cat_params['depth'] = 8\n",
    "cat_params['rsm'] = 0.95\n",
    "cat_params['learning_rate'] = 0.03\n",
    "cat_params['l2_leaf_reg'] = 3.5  \n",
    "cat_params['border_count'] = 8\n",
    "cat_params['gradient_iterations'] = 4\n",
    "\n",
    "\n",
    "# Regularized Greedy Forest params\n",
    "rgf_params = {}\n",
    "rgf_params['max_leaf'] = 2000\n",
    "rgf_params['learning_rate'] = 0.5\n",
    "rgf_params['algorithm'] = \"RGF_Sib\"\n",
    "rgf_params['test_interval'] = 100\n",
    "rgf_params['min_samples_leaf'] = 3 \n",
    "rgf_params['reg_depth'] = 1.0\n",
    "rgf_params['l2'] = 0.5  \n",
    "rgf_params['sl2'] = 0.005\n",
    "\n",
    "\n",
    "\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "#et_model = ExtraTreesClassifier(**et_params)\n",
    "        \n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "\n",
    "rgf_model = RGFClassifier(**rgf_params) \n",
    "\n",
    "gb_model = GradientBoostingClassifier(max_depth=5)\n",
    "\n",
    "#ada_model = AdaBoostClassifier()\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "\n",
    "        \n",
    "stack = Ensemble(n_splits=5,\n",
    "        stacker = log_model,\n",
    "        base_models = (lgb_model, xgb_model, gb_model, rf_model, cat_model, rgf_model))        \n",
    "        \n",
    "y_pred = stack.fit_predict(train, target_train, test)        \n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "#Stacker score: 0.6404 AUC, LB: 0.281 Gini\n",
    "#Stacker score: 0.6420 AUC, LB: 0.282 Gini\n",
    "#Stacker score: 0.64218 AUC, LB: 0.283 Gini\n",
    "#Stacker score: 0.64243 AUC, LB: 0.283 Gini\n",
    "#Stacker score: 0.64268 AUC, LB: 0.284 Gini\n",
    "\n",
    "#############################################\n",
    "       \n",
    "        \n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_pred\n",
    "sub.to_csv('stacked_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
