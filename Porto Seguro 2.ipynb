{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagashin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/sagashin/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/sagashin/anaconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import *\n",
    "from scipy import *\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import re\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold;\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  2 48]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = XGBClassifier()\n",
    "pred = cross_val_predict(clf, X, y)\n",
    "cm = confusion_matrix(y, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the list of documents\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'ps_ind_01':'ps_calc_20_bin'],\n",
    "                      test.loc[:,'ps_ind_01':'ps_calc_20_bin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data.fillna(all_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=30, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=30, n_iter=7, random_state=42)\n",
    "svd.fit(all_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = svd.transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y_train = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "denom = 0\n",
    "fold = 5 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'auc'},\n",
    "        #'num_class': 9,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 28,\n",
    "        'num_leaves': 136,\n",
    "        'min_data_in_leaf': 8,\n",
    "        'lambda_l1': 0.02,\n",
    "        'num_iteration': 2000,\n",
    "        'feature_fraction': 0.8, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq': 5,\n",
    "        'colsample_bytree': 0.3542618105439753\n",
    "        #'verbose': 0\n",
    "    }\n",
    "    X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size=0.18, random_state=i)\n",
    "        \n",
    "    lgb_train = lgb.Dataset(X_train_cv, y_train_cv)\n",
    "    lgb_eval = lgb.Dataset(X_test_cv, y_test_cv, reference=lgb_train)\n",
    "\n",
    "# train\n",
    "    gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            #num_boost_round=rnds,\n",
    "            valid_sets=lgb_eval,\n",
    "            verbose_eval=10,\n",
    "            early_stopping_rounds=20)\n",
    "    \n",
    "    if denom != 0:\n",
    "        pred = gbm.predict(X_test, num_iteration=gbm.best_iteration+50)      \n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = gbm.predict(X_test, num_iteration=gbm.best_iteration+50)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    #submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    #submission = submission.reset_index()\n",
    "    #submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "    #submission['ID'] = pid\n",
    "    #submission.to_csv('submission_xgb_fold_'  + str(i) + '.csv', index=False)\n",
    "    \n",
    "preds /= denom\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['target'] = preds\n",
    "submission.columns = [\"id\",\"target\"]\n",
    "submission.to_csv(\"sub_LGBM.csv\", index=False)\n",
    "#submission['ID'] = pid\n",
    "#y_pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration)\n",
    "#y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(truth, predictions):\n",
    "    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(truth) + 1) / 2.\n",
    "    return gs / len(truth)\n",
    "\n",
    "def gini_xgb(predictions, truth):\n",
    "    truth = truth.get_label()\n",
    "    return 'gini', -1.0 * gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "def gini_lgb(truth, predictions):\n",
    "    score = gini(truth, predictions) / gini(truth, truth)\n",
    "    return 'gini', score, True\n",
    "\n",
    "def gini_sklearn(truth, predictions):\n",
    "    return gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "gini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "        'lambda_l1': '{:.3f}'.format(params['lambda_l1']),\n",
    "    }\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold(5)).mean()\n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 100, 200, 2),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 30, 2),\n",
    "    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 20, 1),\n",
    "    'lambda_l1': hp.uniform('lambda_l1', 0.01, 0.05),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM optimum v1\n",
    "Hyperopt estimated optimum {'colsample_bytree': 0.3542618105439753, 'lambda_l1': 0.020025402620244666, 'max_depth': 28.0, 'min_data_in_leaf': 8.0, 'num_leaves': 136.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        learning_rate=0.05,\n",
    "        nthread=4,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold(5)).mean()\n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    return(score)\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost optimum v1\n",
    "Hyperopt estimated optimum {'colsample_bytree': 0.42220114349224486, 'gamma': 0.22144103013844213, 'max_depth': 6.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "    }\n",
    "    clf = RandomForestClassifier(\n",
    "        n_jobs=4, \n",
    "        class_weight='balanced', \n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n",
    "    \n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    \n",
    "    return(score)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stucking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "#ntrain = 1000\n",
    "#ntest = 1000\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "\n",
    "class XgbWrapper2(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['seed'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "        \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "class LgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.n_estimators = params.pop('n_estimators', 500)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        self.gbdt = lgb.train(self.param, lgb_train, self.n_estimators)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }\n",
    "\n",
    "#Xgboosting parameters\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    #'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    #'eval_metric': 'auc',\n",
    "    'n_estimators': 350\n",
    "}\n",
    "\n",
    "# Light GBM parameters\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'auc'},\n",
    "        #'num_class': 9,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 28,\n",
    "        'num_leaves': 136,\n",
    "        'min_data_in_leaf': 8,\n",
    "        'lambda_l1': 0.02,\n",
    "        'num_iteration': 2000,\n",
    "        'feature_fraction': 0.8, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq': 5,\n",
    "        #'colsample_bytree': 0.3542618105439753\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
    "xg = XgbWrapper2(clf=xgb.XGBClassifier, seed=SEED, params=xgb_params)\n",
    "lg = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, params=lgb_params)\n",
    "#xg = XgbWrapper(seed=SEED, params=xgb_params) \n",
    "#lg = LgbWrapper(seed=SEED, params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()\n",
    "x_train = X_train # Creates an array of the train data\n",
    "x_test = X_test # Creats an array of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "#et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "#rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "#ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "#gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "#svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
    "xg_oof_train, xg_oof_test = get_oof(xg,x_train, y_train, x_test) # Support Vector Classifier\n",
    "lg_oof_train, lg_oof_test = get_oof(lg,x_train, y_train, x_test) # Support Vector Classifier\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate((xg_oof_train, lg_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test, lg_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "denom = 0\n",
    "fold = 5 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'auc'},\n",
    "        #'num_class': 9,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 28,\n",
    "        'num_leaves': 136,\n",
    "        'min_data_in_leaf': 8,\n",
    "        'lambda_l1': 0.02,\n",
    "        'num_iteration': 2000,\n",
    "        'feature_fraction': 0.8, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq': 5,\n",
    "        'colsample_bytree': 0.3542618105439753\n",
    "        #'verbose': 0\n",
    "    }\n",
    "    X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(x_train, y_train, test_size=0.18, random_state=i)\n",
    "        \n",
    "    lgb_train = lgb.Dataset(X_train_cv, y_train_cv)\n",
    "    lgb_eval = lgb.Dataset(X_test_cv, y_test_cv, reference=lgb_train)\n",
    "\n",
    "# train\n",
    "    gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            #num_boost_round=rnds,\n",
    "            valid_sets=lgb_eval,\n",
    "            verbose_eval=10,\n",
    "            early_stopping_rounds=20)\n",
    "    \n",
    "    if denom != 0:\n",
    "        pred = gbm.predict(x_test, num_iteration=gbm.best_iteration+50)      \n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = gbm.predict(x_test, num_iteration=gbm.best_iteration+50)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    #submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    #submission = submission.reset_index()\n",
    "    #submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "    #submission['ID'] = pid\n",
    "    #submission.to_csv('submission_xgb_fold_'  + str(i) + '.csv', index=False)\n",
    "    \n",
    "preds /= denom\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['target'] = preds\n",
    "submission.columns = [\"id\",\"target\"]\n",
    "submission.to_csv(\"sub_LGBM.csv\", index=False)\n",
    "#submission['ID'] = pid\n",
    "#y_pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration)\n",
    "#y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
